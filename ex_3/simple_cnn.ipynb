{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)  \n",
    "#         self.fc1 = nn.Linear(32 * 14 * 14, 128)  \n",
    "#         self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool(x)  \n",
    "#         x = x.view(-1, 32 * 14 * 14) \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.1)  \n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.308306\n",
      "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 3.909317\n",
      "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 2.184709\n",
      "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 2.155690\n",
      "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 2.102313\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.068285\n",
      "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.975598\n",
      "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.926683\n",
      "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.920215\n",
      "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.894750\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.847780\n",
      "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.860749\n",
      "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.788023\n",
      "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.751833\n",
      "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.779747\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.739079\n",
      "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.661044\n",
      "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.663636\n",
      "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.646841\n",
      "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.613819\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.565416\n",
      "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.598981\n",
      "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.536815\n",
      "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.574679\n",
      "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.514836\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.535332\n",
      "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 1.465636\n",
      "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 1.465665\n",
      "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 1.411759\n",
      "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 1.436573\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.407900\n",
      "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 1.418754\n",
      "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 1.342130\n",
      "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 1.382617\n",
      "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 1.369159\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.376818\n",
      "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 1.355374\n",
      "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 1.318012\n",
      "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 1.320374\n",
      "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 1.269733\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.321913\n",
      "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 1.267690\n",
      "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 1.356069\n",
      "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 1.270106\n",
      "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 1.292050\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.313645\n",
      "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 1.224828\n",
      "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 1.241298\n",
      "Train Epoch: 10 [30720/50000 (61%)]\tLoss: 1.267352\n",
      "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 1.218049\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.227734\n",
      "Train Epoch: 11 [10240/50000 (20%)]\tLoss: 1.243884\n",
      "Train Epoch: 11 [20480/50000 (41%)]\tLoss: 1.165391\n",
      "Train Epoch: 11 [30720/50000 (61%)]\tLoss: 1.215287\n",
      "Train Epoch: 11 [40960/50000 (82%)]\tLoss: 1.177482\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1.274235\n",
      "Train Epoch: 12 [10240/50000 (20%)]\tLoss: 1.207538\n",
      "Train Epoch: 12 [20480/50000 (41%)]\tLoss: 1.155954\n",
      "Train Epoch: 12 [30720/50000 (61%)]\tLoss: 1.164490\n",
      "Train Epoch: 12 [40960/50000 (82%)]\tLoss: 1.159099\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1.126537\n",
      "Train Epoch: 13 [10240/50000 (20%)]\tLoss: 1.201732\n",
      "Train Epoch: 13 [20480/50000 (41%)]\tLoss: 1.185763\n",
      "Train Epoch: 13 [30720/50000 (61%)]\tLoss: 1.140882\n",
      "Train Epoch: 13 [40960/50000 (82%)]\tLoss: 1.159011\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1.085469\n",
      "Train Epoch: 14 [10240/50000 (20%)]\tLoss: 1.122276\n",
      "Train Epoch: 14 [20480/50000 (41%)]\tLoss: 1.085711\n",
      "Train Epoch: 14 [30720/50000 (61%)]\tLoss: 1.125903\n",
      "Train Epoch: 14 [40960/50000 (82%)]\tLoss: 1.133489\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1.130555\n",
      "Train Epoch: 15 [10240/50000 (20%)]\tLoss: 1.071042\n",
      "Train Epoch: 15 [20480/50000 (41%)]\tLoss: 1.080179\n",
      "Train Epoch: 15 [30720/50000 (61%)]\tLoss: 1.053046\n",
      "Train Epoch: 15 [40960/50000 (82%)]\tLoss: 1.136128\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1.107141\n",
      "Train Epoch: 16 [10240/50000 (20%)]\tLoss: 1.089015\n",
      "Train Epoch: 16 [20480/50000 (41%)]\tLoss: 1.064854\n",
      "Train Epoch: 16 [30720/50000 (61%)]\tLoss: 1.114076\n",
      "Train Epoch: 16 [40960/50000 (82%)]\tLoss: 1.138899\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1.099864\n",
      "Train Epoch: 17 [10240/50000 (20%)]\tLoss: 1.043008\n",
      "Train Epoch: 17 [20480/50000 (41%)]\tLoss: 1.087358\n",
      "Train Epoch: 17 [30720/50000 (61%)]\tLoss: 1.096379\n",
      "Train Epoch: 17 [40960/50000 (82%)]\tLoss: 1.043925\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1.056037\n",
      "Train Epoch: 18 [10240/50000 (20%)]\tLoss: 1.033764\n",
      "Train Epoch: 18 [20480/50000 (41%)]\tLoss: 1.052895\n",
      "Train Epoch: 18 [30720/50000 (61%)]\tLoss: 1.022733\n",
      "Train Epoch: 18 [40960/50000 (82%)]\tLoss: 1.088179\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1.026297\n",
      "Train Epoch: 19 [10240/50000 (20%)]\tLoss: 1.036014\n",
      "Train Epoch: 19 [20480/50000 (41%)]\tLoss: 1.052698\n",
      "Train Epoch: 19 [30720/50000 (61%)]\tLoss: 1.062765\n",
      "Train Epoch: 19 [40960/50000 (82%)]\tLoss: 1.004448\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 1.033045\n",
      "Train Epoch: 20 [10240/50000 (20%)]\tLoss: 1.037786\n",
      "Train Epoch: 20 [20480/50000 (41%)]\tLoss: 0.935964\n",
      "Train Epoch: 20 [30720/50000 (61%)]\tLoss: 1.072890\n",
      "Train Epoch: 20 [40960/50000 (82%)]\tLoss: 1.074022\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1.070504\n",
      "Train Epoch: 21 [10240/50000 (20%)]\tLoss: 0.983577\n",
      "Train Epoch: 21 [20480/50000 (41%)]\tLoss: 0.983009\n",
      "Train Epoch: 21 [30720/50000 (61%)]\tLoss: 0.968858\n",
      "Train Epoch: 21 [40960/50000 (82%)]\tLoss: 1.015389\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.988229\n",
      "Train Epoch: 22 [10240/50000 (20%)]\tLoss: 0.968843\n",
      "Train Epoch: 22 [20480/50000 (41%)]\tLoss: 0.971909\n",
      "Train Epoch: 22 [30720/50000 (61%)]\tLoss: 0.960362\n",
      "Train Epoch: 22 [40960/50000 (82%)]\tLoss: 1.008578\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.942815\n",
      "Train Epoch: 23 [10240/50000 (20%)]\tLoss: 0.989807\n",
      "Train Epoch: 23 [20480/50000 (41%)]\tLoss: 0.982958\n",
      "Train Epoch: 23 [30720/50000 (61%)]\tLoss: 0.977674\n",
      "Train Epoch: 23 [40960/50000 (82%)]\tLoss: 0.962053\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 1.011782\n",
      "Train Epoch: 24 [10240/50000 (20%)]\tLoss: 0.945807\n",
      "Train Epoch: 24 [20480/50000 (41%)]\tLoss: 1.007863\n",
      "Train Epoch: 24 [30720/50000 (61%)]\tLoss: 1.024688\n",
      "Train Epoch: 24 [40960/50000 (82%)]\tLoss: 0.946120\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.917954\n",
      "Train Epoch: 25 [10240/50000 (20%)]\tLoss: 0.979052\n",
      "Train Epoch: 25 [20480/50000 (41%)]\tLoss: 0.955914\n",
      "Train Epoch: 25 [30720/50000 (61%)]\tLoss: 0.958480\n",
      "Train Epoch: 25 [40960/50000 (82%)]\tLoss: 0.968841\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.904566\n",
      "Train Epoch: 26 [10240/50000 (20%)]\tLoss: 0.919799\n",
      "Train Epoch: 26 [20480/50000 (41%)]\tLoss: 0.960596\n",
      "Train Epoch: 26 [30720/50000 (61%)]\tLoss: 0.940794\n",
      "Train Epoch: 26 [40960/50000 (82%)]\tLoss: 0.983059\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.903609\n",
      "Train Epoch: 27 [10240/50000 (20%)]\tLoss: 0.957420\n",
      "Train Epoch: 27 [20480/50000 (41%)]\tLoss: 0.965035\n",
      "Train Epoch: 27 [30720/50000 (61%)]\tLoss: 0.965336\n",
      "Train Epoch: 27 [40960/50000 (82%)]\tLoss: 0.935706\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.951385\n",
      "Train Epoch: 28 [10240/50000 (20%)]\tLoss: 0.930928\n",
      "Train Epoch: 28 [20480/50000 (41%)]\tLoss: 0.879984\n",
      "Train Epoch: 28 [30720/50000 (61%)]\tLoss: 0.932595\n",
      "Train Epoch: 28 [40960/50000 (82%)]\tLoss: 0.851751\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.925991\n",
      "Train Epoch: 29 [10240/50000 (20%)]\tLoss: 0.922345\n",
      "Train Epoch: 29 [20480/50000 (41%)]\tLoss: 0.943435\n",
      "Train Epoch: 29 [30720/50000 (61%)]\tLoss: 0.913985\n",
      "Train Epoch: 29 [40960/50000 (82%)]\tLoss: 0.908942\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     datasets.MNIST('./data', train=True, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=64, shuffle=True)\n",
    "\n",
    "# 数据加载和预处理\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.226, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10('./data/CIFAR10', train=True, download=True, transform=transforms_train),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "# 模型实例化和优化器设置\n",
    "model = SimpleCNN()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=0.01)\n",
    "\n",
    "# 设备设置\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(1, 30):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# # 保存模型\n",
    "# torch.save(model.state_dict(), f'./model/mnist_cnn.pt')\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), './model/cifar10_cnn.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Test set: Average loss: 0.9623, Accuracy: 6632/10000 (66%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test.py\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "# 数据加载\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.226, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    datasets.CIFAR10('./data/CIFAR10', train=False, download=True, transform=transforms_train),\n",
    "    batch_size=1000, shuffle=True)\n",
    "\n",
    "model = SimpleCNN()\n",
    "# model.load_state_dict(torch.load('./model/mnist_cnn.pt'))\n",
    "model.load_state_dict(torch.load('./model/cifar10_cnn.pt'))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
